{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new experiment after faiss fails to index the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag, word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/abhay/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/abhay/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/abhay/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 105 Q&A pairs from the text.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_single_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def split_text_into_qa_pairs(text):\n",
    "    qa_pairs = text.split(\"\\n\\n\")\n",
    "    return qa_pairs\n",
    "\n",
    "\n",
    "file_path = \"/home/abhay/my_projects/ps2_chatbot/faculty_info/faculty_info_iet/iet_qa/all_faculty.txt\" \n",
    "text_data = load_single_file(file_path)\n",
    "qa_pairs = split_text_into_qa_pairs(text_data)\n",
    "\n",
    "print(f\"Extracted {len(qa_pairs)} Q&A pairs from the text.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created.\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "def generate_qa_embeddings(qa_pairs, model):\n",
    "    embeddings = model.encode(qa_pairs)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "qa_embeddings = generate_qa_embeddings(qa_pairs, model)\n",
    "\n",
    "\n",
    "def create_faiss_index(embeddings):\n",
    "    dimension = embeddings.shape[1]  \n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(np.array(embeddings))  \n",
    "    return index\n",
    "\n",
    "\n",
    "index = create_faiss_index(qa_embeddings)\n",
    "print(\"FAISS index created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded Query: rabia kamra\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def expand_query_with_synonyms_limited(query):\n",
    "    expanded_query = set([query])\n",
    "    words = word_tokenize(query)\n",
    "    tagged_words = pos_tag(words)\n",
    "\n",
    "    for word, tag in tagged_words:\n",
    "        if tag.startswith('N') or tag.startswith('V'): \n",
    "            for syn in wordnet.synsets(word):\n",
    "                for lemma in syn.lemmas():\n",
    "                    expanded_query.add(lemma.name().replace('_', ' '))\n",
    "    \n",
    "    return \" \".join(expanded_query)\n",
    "\n",
    "\n",
    "query = \"rabia kamra\"\n",
    "expanded_query = expand_query_with_synonyms_limited(query)\n",
    "print(f\"Expanded Query: {expanded_query}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top relevant Q&A pairs:\n",
      "----------------------------------------\n",
      "Q: Who is Dr. Rabia Kamra at JKLU?\n",
      "A: Dr. Rabia Kamra is an Assistant Professor in the Department of Science and Liberal Arts at JKLU. She holds a PhD from IIT Delhi and specializes in designing parallel algorithms for large linear systems.\n",
      "----------------------------------------\n",
      "Mathematics - Dr. Rabia Kamra\n",
      "----------------------------------------\n",
      "Q: What are Dr. Rabia Kamra's research interests?\n",
      "A: Numerical Analysis and Parallel Computing.\n",
      "----------------------------------------\n",
      "Q: What are Dr. Rabia Kamra's teaching interests?\n",
      "A: Numerical Methods, Calculus, Ordinary Differential Equations, and Matrix Computations.\n",
      "----------------------------------------\n",
      "Q: Who is Dr. Surbhi Chhabra at JKLU?\n",
      "A: Dr. Surbhi Chhabra is an Assistant Professor of Electronics and Communication Engineering at JKLU. She holds a PhD in Hardware Security from The LNM Institute of Information Technology, Jaipur.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def search_faiss_index(query, index, qa_pairs, model, top_k=5):\n",
    "    query_embedding = model.encode([query])\n",
    "    distances, indices = index.search(np.array(query_embedding), top_k)\n",
    "    relevant_qas = [qa_pairs[i] for i in indices[0]]  \n",
    "    return relevant_qas\n",
    "\n",
    "\n",
    "relevant_qas = search_faiss_index(expanded_query, index, qa_pairs, model, top_k=5)\n",
    "\n",
    "\n",
    "print(f\"Top relevant Q&A pairs:\\n{'-'*40}\")\n",
    "for qa in relevant_qas:\n",
    "    print(f\"{qa}\\n{'-'*40}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response:\n",
      "Dr. Rabia Kamra is an Assistant Professor in the Department of Science and Liberal Arts at JKLU. She holds a PhD from IIT Delhi and specializes in designing parallel algorithms for large linear systems. Mathematical Methods\n"
     ]
    }
   ],
   "source": [
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-large')\n",
    "\n",
    "\n",
    "def generate_summary_with_t5(context, query, t5_tokenizer, t5_model):\n",
    "    input_text = f\"question: {query} context: {context}\"\n",
    "    inputs = t5_tokenizer.encode(input_text, return_tensors='pt', max_length=512, truncation=True)\n",
    "    \n",
    "    summary_ids = t5_model.generate(inputs, max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "\n",
    "context = \" \".join(relevant_qas)\n",
    "\n",
    "\n",
    "response = generate_summary_with_t5(context, query, t5_tokenizer, t5_model)\n",
    "print(f\"Generated Response:\\n{response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
